---
title: "data621 assignment 5"
author: "Joby John, Jun Pan"
date: "April 25, 2019"
output: html_document
---

In this we will explore, analyze and model a data set containing information on
approximately 12,000 commercially available wines. The variables are mostly related to the chemical properties of
the wine being sold. The response variable is the number of sample cases of wine that were purchased by wine
distribution companies after sampling a wine. These cases would be used to provide tasting samples to
restaurants and wine stores around the United States. The more sample cases purchased, the more likely is a
wine to be sold at a high end restaurant. Our objective is to build a count regression model to predict the number of 
cases of wine that will be sold given certain properties of the wine.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(car)
library(caret)
library(caTools)
library(corrplot)
library(data.table)
library(dplyr)
library(geoR)
library(ggthemes)
library(ggplot2)
library(grid)
library(gridExtra)
library(kableExtra)
library(knitr)
library(MASS)
library(mice)
library(naniar)
library(nortest)
library(pscl)
library(psych)
library(testthat)
library(ggthemes)
library(Hmisc)
library(tidyverse)
library(tidyr)
```

```{r}
train <- read.csv("https://raw.githubusercontent.com/jjohn81/DATA621_Assignment_5/master/wine-training-data.csv")
eval <- read.csv("https://raw.githubusercontent.com/jjohn81/DATA621_Assignment_5/master/wine-evaluation-data.csv")
train$INDEX <-NULL
```

The dataset contains ```nrow(train)``` observations and ```ncol(train)``` variables.
All  independent variables are continous, except   `LabelAppeal`, `AcidIndex` and `STARS` which are ordinal.These three ordinal variables represented by numeric values in logical order. The distances between categories/values can be considered equal,so these variables can be used in modeling as numeric variables. 
Below table displays summary of the data.
 
```{r}
summary(train)
```

 

```{r}
hist.data.frame(train)
```

```{r}
sapply(train, function(x) sum(is.na(x))) %>% kable() %>% kable_styling()
```




```{r}
summary_metrics <- function(df){
  ###Creates summary metrics table
  metrics_only <- df[, sapply(df, is.numeric)]
   
  df_metrics <- psych::describe(metrics_only, quant = c(.25,.75))
  df_metrics$unique_values = rapply(metrics_only, function(x) length(unique(x)))
  df_metrics <- 
    dplyr::select(df_metrics, n, unique_values, min, Q.1st = Q0.25, median, mean, Q.3rd = Q0.75, 
    max, range, sd, skew, kurtosis
  )
  return(df_metrics)
}


metrics_df <- summary_metrics(train)

boxplot_data <- 
  train %>% 
  dplyr::select(rownames(metrics_df)[metrics_df$unique_values < 15]) %>% 
  reshape2::melt(id.vars = "TARGET")

ggplot(data = boxplot_data, aes(x = factor(value), y = TARGET)) +
  geom_boxplot() +
  facet_wrap( ~ variable, scales = "free") +
  coord_flip() +
  ggthemes::theme_fivethirtyeight()
```


```{r}
train_clean<- mice(train,m=1,maxit=1)
```


```{r}
densityplot(train_clean)
```


```{r}
df.train.nomissing <- mice::complete(train_clean)
```


```{r}
df.train.nooutliers <- df.train.nomissing

id <- c(1:15)
for (val in id) {
  qnt <- quantile(df.train.nooutliers[,val], probs=c(.25, .75), na.rm = T)
  caps <- quantile(df.train.nooutliers[,val], probs=c(.05, .95), na.rm = T)
  H <- 1.5 * IQR(df.train.nooutliers[,val], na.rm = T)
  df.train.nooutliers[,val][df.train.nooutliers[,val] < (qnt[1] - H)] <- caps[1]
  df.train.nooutliers[,val][df.train.nooutliers[,val] > (qnt[2] + H)] <- caps[2]
}

df.train.transformed <- df.train.nooutliers

```




```{r}
train1 <- df.train.transformed
```






```{r}
corrplot(cor(train1~.-TARGET), method="square")
```


split train dataset
```{r}
set.seed(999) 
sampl = sample.split(train1$TARGET, SplitRatio = .80)
wine_train1 <- subset(train1, sampl == TRUE)
wine_test1 <- subset(train1, sampl == FALSE)
```


	
```{r}
dim(wine_train1)
```


```{r}
dim(wine_test1)
```

#### Models
variance and mean is not same so we need to either quasi or negative bionomial regression.

```{r}
mean(train$TARGET)
var(train$TARGET)
hist(train$TARGET)

```

#####Poisson, Full model
```{r}
full_model = glm(TARGET ~  ., data=wine_train1, family=poisson)
summary(full_model)
qchisq(0.95, df.residual(full_model))
deviance(full_model)
 pr <- residuals(full_model,"pearson")
  sum(pr^2)
   phi <- sum(pr^2)/df.residual(full_model)

  round(c(phi,sqrt(phi)),4)
```
 
#####Negative binomial model and stepwise
```{r}
negative_binomial_model <- glm.nb(TARGET~., data=wine_train1)

step_neg_binomial_model <- step(negative_binomial_model)
summary(negative_binomial_model)
```

#####Quasi-Poisson Model and stepwise

Step wise wouldnt work here because qausipossion doesnt have likelihood  function so will select a model with only signficant variables. 
```{r}
 
quasi_p_model <- glm(TARGET~., data=wine_train1, family=quasipoisson)
summary(quasi_p_model)
sig_quasi_p_model <- glm(TARGET~.-FixedAcidity-CitricAcid-Alcohol, data=wine_train1, family=quasipoisson)
```

Using the squared loss to validate the model.

```{r}
modelValidation <- function(mod, test){
  preds = predict(mod, test)
  diffMat = as.numeric(preds) - as.numeric(test$TARGET)
  diffMat = diffMat^2
  loss <- mean(diffMat)
  return(loss)
}
```

```{r}
eval$Target_Possion <- predict(negative_binomial_model, type="response", eval)
eval$Target_negative_binomial_model <- predict(negative_binomial_model, type="response", eval)
eval$Target_step_negative_binomial_model <- predict(step_neg_binomial_model, type="response",eval)
eval$Target_quasi_p_model <- predict(quasi_p_model, type="response",eval)
eval$Target_step_quasi_p_model <- predict(sig_quasi_p_model, type="response",eval)

 hist(eval$Target_Possion)
 hist(eval$Target_negative_binomial_model)
 hist(eval$Target_step_negative_binomial_model)
 hist(eval$Target_quasi_p_model)
 hist(eval$Target_step_quasi_p_model)
```
